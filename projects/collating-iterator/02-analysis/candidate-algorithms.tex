\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\usepackage[margin=1in]{geometry}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{hyperref}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{invariant}{Invariant}

\title{Collating Iterator: Candidate Algorithms}
\author{Research Artifact - Stage 2B}
\date{October 2025}

\begin{document}
\maketitle

\section{Research Question}

From Stage 1 and Stage 2A: \textit{``Which algorithms (if any) achieve the $\Omega(N \log k)$ lower bound?''}

We know from Stage 2A that $\Omega(N \log k)$ comparisons are required. Now we explore which algorithms achieve this bound.

\section{Literature Review}

\textbf{Sources consulted:}
\begin{itemize}
    \item \textbf{Knuth TAOCP Vol 3 §5.4.1}: K-way merge using heap, tournament tree, loser tree
    \item \textbf{CLRS Ch 6, Problem 6-2}: Binary heaps, d-ary heap variants
    \item \textbf{Wikipedia}: K-way merge algorithm overview
    \item \textbf{Grafana Labs blog (2024)}: Practical loser tree implementations
    \item \textbf{Frigo et al. (1999)}: Cache-oblivious funnelsort
    \item \textbf{WebSearch}: Modern surveys and Stack Overflow discussions
\end{itemize}

\textbf{Algorithms discovered in literature:}
\begin{enumerate}
    \item Linear scan (naive baseline)
    \item Binary min-heap / priority queue
    \item Winner tournament tree
    \item \textbf{Loser tournament tree} (Knuth preferred, used in production)
    \item D-ary heap (d=4 common in practice)
    \item Pairwise merge (divide-and-conquer)
    \item Collect-and-sort (ignores pre-sorted property)
    \item Cache-oblivious funnelsort
\end{enumerate}

\section{Candidate Approaches}

Without assuming the answer, we analyze eight natural approaches:

\subsection{Candidate 1: Linear Scan}

\textbf{Source}: First principles (naive baseline)

\subsubsection{Algorithm}

\begin{algorithm}
\caption{Linear Scan K-Way Merge}
\begin{algorithmic}[1]
\State \textbf{next()}:
\State $\text{minVal} \gets \infty$
\State $\text{minIdx} \gets -1$
\For{$i \gets 0$ to $k-1$}
    \If{$\text{iterators}[i].\texttt{hasNext()}$}
        \State $\text{val} \gets \text{iterators}[i].\texttt{peek()}$
        \If{$\text{val} < \text{minVal}$}
            \State $\text{minVal} \gets \text{val}$
            \State $\text{minIdx} \gets i$
        \EndIf
    \EndIf
\EndFor
\State $\text{iterators}[\text{minIdx}].\texttt{next()}$
\State \Return $\text{minVal}$
\end{algorithmic}
\end{algorithm}

\subsubsection{Analysis}

\textbf{Time Complexity}:
\begin{itemize}
    \item Per \texttt{next()}: $O(k)$ (scan all iterators)
    \item Total: $O(Nk)$ for $N$ elements
\end{itemize}

\textbf{Space Complexity}: $O(k)$ (iterator references only)

\textbf{Comparison to Lower Bound}:
\[
O(Nk) \gg \Omega(N \log k) \quad \text{for } k > \log k
\]

\textbf{Achieves bound?} \textcolor{red}{NO} - Sub-optimal asymptotically for $k > $ constant

\textbf{When Competitive}: For small $k$ (say $k \leq 8$), the simplicity and cache locality may make this competitive despite poor asymptotics.

\subsection{Candidate 2: Priority Queue (Binary Min-Heap)}

\textbf{Source}: Standard (CLRS Ch 6, TAOCP Vol 3 §5.4.1)

\subsubsection{Algorithm Idea}

Maintain a priority queue (min-heap) containing:
\begin{itemize}
    \item One element from each non-empty iterator
    \item Each entry: (element value, source iterator)
\end{itemize}

Operations:
\begin{itemize}
    \item \texttt{extractMin()}: Remove minimum from queue
    \item Refill: Advance source iterator, insert next element
\end{itemize}

\subsubsection{Binary Min-Heap Implementation}

\begin{algorithm}
\caption{Priority Queue K-Way Merge}
\begin{algorithmic}[1]
\State \textbf{Initialize}:
\State $H \gets \text{empty min-heap}$
\For{each iterator $I_i$}
    \If{$I_i.\texttt{hasNext()}$}
        \State $H.\texttt{insert}((I_i.\texttt{next}(), I_i))$
    \EndIf
\EndFor
\State
\State \textbf{next()}:
\State $(val, iter) \gets H.\texttt{extractMin}()$
\If{$iter.\texttt{hasNext()}$}
    \State $H.\texttt{insert}((iter.\texttt{next}(), iter))$
\EndIf
\State \Return $val$
\end{algorithmic}
\end{algorithm}

\subsubsection{Analysis}

\textbf{Heap Operations}:
\begin{itemize}
    \item \texttt{insert()}: $O(\log k)$ (sift-up)
    \item \texttt{extractMin()}: $O(\log k)$ (sift-down)
    \item Heap size: At most $k$ elements
\end{itemize}

\textbf{Time Complexity}:
\begin{itemize}
    \item Initialization: $O(k \log k)$ (or $O(k)$ with heapify)
    \item Per \texttt{next()}: $O(\log k)$
    \item Total: $O(k \log k + N \log k) = O(N \log k)$
\end{itemize}

\textbf{Space Complexity}: $O(k)$ (heap storage)

\textbf{Comparison to Lower Bound}: $O(N \log k) = \Omega(N \log k)$

\textbf{Achieves bound?} \textcolor{green}{YES ✓} - Asymptotically optimal!

\subsubsection{Correctness}

\begin{invariant}[Heap Invariant]
At the start of each \texttt{next()} call:
\begin{enumerate}
    \item $H$ contains at most one element from each non-exhausted iterator
    \item Each element in $H$ is the next unconsumed element from its source iterator
    \item All elements output so far are $\leq$ all elements in $H$
\end{enumerate}
\end{invariant}

\begin{proof}[Correctness Proof]
\textbf{Initialization}: Insert first element from each iterator. By definition, first elements are minimums of their iterators. Invariant holds.

\textbf{Maintenance}:
\begin{itemize}
    \item Extract minimum $v$ from $H$
    \item By heap property: $v \leq $ all other elements in $H$
    \item By invariant (2): Elements in $H$ are minimums of their iterators
    \item Therefore: $v \leq $ all remaining elements globally
    \item Refill from source iterator: New element is next minimum from that iterator
    \item Invariant maintained
\end{itemize}

\textbf{Termination}: $H$ becomes empty iff all iterators exhausted. All $N$ elements extracted in sorted order. $\square$
\end{proof}

\subsection{Candidate 3: Winner Tournament Tree}

\textbf{Source}: TAOCP Vol 3 §5.4.1 ``tournament sort''

\subsubsection{Algorithm Idea}

Build a complete binary tree with:
\begin{itemize}
    \item $k$ leaves (one per iterator)
    \item Each internal node: minimum of its children (``winner'')
    \item Root: global minimum
\end{itemize}

\subsubsection{Analysis}

\textbf{Time Complexity}:
\begin{itemize}
    \item Per \texttt{next()}: Extract root, refill leaf, propagate up
    \item Propagation cost: $O(\log k)$ (tree height)
    \item Total: $O(N \log k)$
\end{itemize}

\textbf{Space Complexity}: $O(k)$ (tree nodes)

\textbf{Comparison to Lower Bound}: $O(N \log k) = \Omega(N \log k)$

\textbf{Achieves bound?} \textcolor{green}{YES ✓} - Asymptotically optimal!

\textbf{Comparison to Heap}: Same asymptotic complexity. Constant factors differ (see Stage 3).

\subsection{Candidate 4: Loser Tournament Tree}

\textbf{Source}: TAOCP Vol 3 §5.4.1 (Knuth's preferred variant), Grafana Labs production use (2024)

\subsubsection{Algorithm Idea}

A variant of tournament tree where:
\begin{itemize}
    \item Internal nodes store the \textbf{loser} of each comparison
    \item Winner propagates up to next level
    \item Root pointer indicates overall winner
\end{itemize}

\textbf{Key difference from winner tree}: During refill, only need to compare against losers on path to root, not recompute entire subtree.

\subsubsection{Analysis}

\textbf{Time Complexity}:
\begin{itemize}
    \item Per \texttt{next()}: Read winner, refill leaf, compare against losers up path
    \item Comparisons: Exactly $\log_2 k$ (one per level)
    \item Total: $O(N \log k)$
\end{itemize}

\textbf{Space Complexity}: $O(k)$ (tree nodes + root pointer)

\textbf{Comparison to Lower Bound}: $O(N \log k) = \Omega(N \log k)$

\textbf{Achieves bound?} \textcolor{green}{YES ✓} - Asymptotically optimal!

\textbf{Advantages over Winner Tree}:
\begin{itemize}
    \item Simpler refill logic (only compare with losers, not siblings)
    \item Fewer branches in code
    \item Better for expensive comparison operations
    \item Used in production (Grafana Loki log aggregation system)
\end{itemize}

\textbf{Disadvantages}:
\begin{itemize}
    \item More complex to implement than binary heap
    \item Pointer-based structure (worse cache behavior than heap)
\end{itemize}

\subsection{Candidate 5: D-ary Heap}

\textbf{Source}: CLRS Problem 6-2, generalization of binary heap

\subsubsection{Algorithm Idea}

Generalization of binary heap where each node has $d$ children instead of 2.

\textbf{Operations}:
\begin{itemize}
    \item \texttt{extractMin()}: $(d-1) \log_d k$ comparisons (compare $d$ children per level)
    \item Tree height: $\log_d k$
\end{itemize}

\subsubsection{Analysis}

\textbf{Time Complexity}:
\begin{itemize}
    \item Per \texttt{next()}: $O(d \log_d k) = O(d \frac{\log k}{\log d})$
    \item For fixed $d$: $O(\log k)$
    \item Total: $O(N \log k)$
\end{itemize}

\textbf{Space Complexity}: $O(k)$ (array storage)

\textbf{Trade-off}:
\begin{itemize}
    \item Larger $d$ $\rightarrow$ shallower tree (fewer levels)
    \item But: more comparisons per level ($d-1$ instead of 2)
    \item Optimal $d$ depends on cache behavior and comparison cost
\end{itemize}

\textbf{Practical Choice}: $d=4$ commonly used (.NET PriorityQueue uses quaternary heap)

\textbf{Achieves bound?} \textcolor{green}{YES ✓} - Still $O(N \log k)$

\subsection{Candidate 6: Pairwise Merge}

\textbf{Source}: Divide-and-conquer mergesort pattern

\subsubsection{Algorithm Idea}

Recursively merge iterators in pairs:
\[
((I_0 \oplus I_1) \oplus (I_2 \oplus I_3)) \oplus \ldots
\]

\subsubsection{Analysis}

\textbf{Problem}: Violates lazy evaluation requirement!

Pairwise merging forces materialization of intermediate results. For example, $(I_0 \oplus I_1)$ must complete before merging with $(I_2 \oplus I_3)$.

\textbf{Time Complexity}: $O(N \log k)$ if we materialize intermediates.

\textbf{Space Complexity}: $O(N)$ (must store intermediate merged sequences).

\textbf{Achieves bound?} \textcolor{red}{NO} - Violates problem constraints (lazy evaluation). Space requirement $O(N) \gg O(k)$.

\textbf{When Useful}: Parallel merging (divide-and-conquer parallelizes naturally). Different problem domain.

\subsection{Candidate 7: Collect-and-Sort}

\textbf{Source}: First principles (ignores pre-sorted property)

\subsubsection{Algorithm}

\begin{algorithmic}
\State Consume all $k$ iterators into array $A$
\State Sort $A$
\State Return sequential iterator over $A$
\end{algorithmic}

\subsubsection{Analysis}

\textbf{Time Complexity}: $O(N \log N)$

\textbf{Space Complexity}: $O(N)$ (must store all elements)

\textbf{Comparison to Lower Bound}:
\[
O(N \log N) \gg O(N \log k) \quad \text{when } k \ll N
\]

\textbf{Achieves bound?} \textcolor{red}{NO} - Sub-optimal. Doesn't exploit pre-sorted property of inputs. Violates lazy evaluation and space constraints.

\subsection{Candidate 8: Cache-Oblivious Funnelsort}

\textbf{Source}: Frigo, Leiserson, Prokop, and Ramachandran (1999)

\subsubsection{Algorithm Idea}

A cache-oblivious sorting algorithm designed for external memory:
\begin{itemize}
    \item Recursive k-way merge with ``funnels''
    \item Adapts to memory hierarchy without knowing cache parameters
    \item Optimal I/O complexity in external memory model
\end{itemize}

\subsubsection{Analysis}

\textbf{Time Complexity}: $O(N \log N)$ comparisons (for full sort, not just k-way merge)

\textbf{I/O Complexity}: $O(\frac{N}{B} \log_{M/B} \frac{N}{B})$ where $B$ = block size, $M$ = memory size

\textbf{Relevance to Our Problem}:
\begin{itemize}
    \item Designed for full sorting, not k-way merge of pre-sorted sequences
    \item Cache-oblivious property interesting but not directly applicable
    \item Complexity doesn't improve on simple k-way merge for our constraints
\end{itemize}

\textbf{Achieves bound?} \textcolor{red}{NO} - Different problem domain (external memory sorting vs lazy k-way merge)

\textbf{Note}: Interesting for Stage 7 related work discussion

\section{Summary of Candidate Algorithms}

\begin{table}[h]
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Algorithm} & \textbf{Time} & \textbf{Space} & \textbf{Lazy?} & \textbf{Optimal?} & \textbf{Source} \\
\midrule
Linear Scan & $O(Nk)$ & $O(k)$ & Yes & No & Baseline \\
Binary Heap & $O(N \log k)$ & $O(k)$ & Yes & \textbf{Yes} & CLRS/TAOCP \\
Winner Tree & $O(N \log k)$ & $O(k)$ & Yes & \textbf{Yes} & TAOCP §5.4.1 \\
Loser Tree & $O(N \log k)$ & $O(k)$ & Yes & \textbf{Yes} & TAOCP §5.4.1 \\
D-ary Heap & $O(N \log k)$ & $O(k)$ & Yes & \textbf{Yes} & CLRS Prob 6-2 \\
Pairwise & $O(N \log k)$ & $O(N)$ & No & No & Divide-conquer \\
Collect-Sort & $O(N \log N)$ & $O(N)$ & No & No & Naive \\
Funnelsort & $O(N \log N)$ & Varies & No & No & Frigo et al. \\
\bottomrule
\end{tabular}
\caption{Comparison of candidate algorithms}
\end{table}

\section{Constant Factor Analysis}

For the four optimal candidates:

\begin{table}[h]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Algorithm} & \textbf{Comparisons per \texttt{next()}} & \textbf{Memory Pattern} \\
\midrule
Binary Heap & $2 \log_2 k$ & Array (excellent cache) \\
Winner Tree & $\log_2 k$ & Pointer-based (poor cache) \\
Loser Tree & $\log_2 k$ & Pointer-based (poor cache) \\
D-ary Heap (d=4) & $3 \log_4 k \approx 1.5 \log_2 k$ & Array (excellent cache) \\
\bottomrule
\end{tabular}
\caption{Constant factors for optimal algorithms}
\end{table}

\textbf{Observations}:
\begin{itemize}
    \item Winner/loser trees: Fewer comparisons but worse memory access patterns
    \item Heaps: More comparisons but better cache locality
    \item Modern processors: Memory access often dominates comparison cost
    \item D-ary heap: Best of both worlds? (fewer comparisons + array storage)
\end{itemize}

\section{Conclusion}

\textbf{Discovery}: Four algorithm families achieve $\Omega(N \log k)$ lower bound:
\begin{enumerate}
    \item Binary min-heap (standard, simple, good cache behavior)
    \item Winner tournament tree (fewer comparisons, pointer-based)
    \item \textbf{Loser tournament tree} (Knuth preferred, production-proven, simpler refill)
    \item D-ary heap (trade-off parameter $d$, d=4 common)
\end{enumerate}

\textbf{Linear scan} competitive for $k \leq 8$ despite poor asymptotics.

\textbf{Open Question}: Which is better in practice? (heap vs loser tree vs d-ary heap)

This constant factor analysis is deferred to \textbf{Stage 3: Design Selection}.

\end{document}
