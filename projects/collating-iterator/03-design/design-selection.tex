\documentclass[11pt]{article}
\usepackage{amsmath,amssymb}
\usepackage[margin=1in]{geometry}
\usepackage{listings}

\title{Collating Iterator: Design Selection and Justification}
\author{Research Artifact}
\date{October 2025}

\begin{document}
\maketitle

\section{Design Choice: Array-Based Binary Min-Heap}

Based on the comparative analysis, we select the \textbf{array-based binary min-heap} as the primary data structure for CollatingIterator.

\section{Rationale}

\subsection{Asymptotic Optimality}

The heap-based approach achieves $O(N \log k)$ total time complexity, matching the comparison-based lower bound. This is the best possible asymptotic performance for this problem.

\subsection{Constant Factor Excellence}

\begin{itemize}
    \item Comparisons per element: $\sim 2 \log_2 k$ (practical and measurable)
    \item Memory accesses: $\sim 3 \log_2 k$ (cache-friendly array traversal)
    \item No allocations per element (reuse fixed-size array)
    \item Branch mispredictions: $\log_2 k$ (unavoidable in comparison-based sorting)
\end{itemize}

\subsection{Cache Behavior}

\textbf{Key Advantage}: Array-based heap exhibits excellent spatial locality.

\begin{itemize}
    \item \textbf{Contiguous Storage}: Heap stored in single array, no pointer chasing
    \item \textbf{Prefetcher-Friendly}: Sequential access patterns during sift operations
    \item \textbf{Cache Resident}: For $k = 100$, heap occupies $\sim 2.4$KB, fits entirely in L1 cache
    \item \textbf{Predictable Strides}: Parent-child relationships via arithmetic (no indirection)
\end{itemize}

Contrast with tournament tree:
\begin{itemize}
    \item Pointer-based: Each node access potentially a cache miss
    \item Scattered allocation: Nodes may be far apart in memory
    \item $k = 100$ tournament: $\sim 50$ cache lines vs $\sim 13$ for heap
\end{itemize}

\subsection{Implementation Simplicity}

\textbf{Heap Advantages}:
\begin{itemize}
    \item Well-known data structure (CLRS Chapter 6, Knuth TAOCP Vol 3)
    \item Simple invariants (heap property easily verified)
    \item Standard library support (Java PriorityQueue, C++ priority\_queue, Rust BinaryHeap)
    \item Debuggable (array visualization straightforward)
\end{itemize}

\textbf{Estimated Effort}:
\begin{itemize}
    \item Custom implementation: $\sim 60$ lines of code
    \item Standard library usage: $\sim 20$ lines of code
    \item Test burden: Medium (edge cases: empty heap, single element, duplicates)
\end{itemize}

\subsection{Scalability}

The heap-based approach scales gracefully:

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\hline
$k$ & Comparisons/elem & Heap Size & Cache Level \\ \hline
2   & 3                & 48B       & L1          \\
10  & 10               & 240B      & L1          \\
100 & 20               & 2.4KB     & L1          \\
1000 & 30              & 24KB      & L2          \\
10000 & 40             & 240KB     & L2/L3       \\ \hline
\end{tabular}
\caption{Scalability characteristics}
\end{table}

Even for $k = 10000$, performance remains $O(\log k) = O(40)$ operations per element, vastly superior to $O(k) = O(10000)$.

\section{Heap Entry Design}

Each heap entry contains:

\begin{lstlisting}[language=Java, basicstyle=\small\ttfamily]
class HeapEntry<T extends Comparable<T>> {
    T element;              // Current minimum from this iterator
    Iterator<T> iterator;   // Source iterator reference
    int index;              // Iterator index (for tie-breaking)
}
\end{lstlisting}

\textbf{Design Decisions}:
\begin{itemize}
    \item \textbf{element}: Cached minimum, avoid redundant iterator.next() calls
    \item \textbf{iterator}: Reference to source, enables refill after extraction
    \item \textbf{index}: Enables stable sorting (tie-breaking) if needed in future
\end{itemize}

\textbf{Memory Layout} (64-bit JVM):
\begin{itemize}
    \item Object header: 12 bytes (mark word + klass pointer)
    \item element reference: 8 bytes (compressed oops)
    \item iterator reference: 8 bytes
    \item index int: 4 bytes
    \item Padding: 4 bytes (alignment)
    \item Total: 36 bytes per entry
\end{itemize}

For $k = 100$: $100 \times 36 = 3.6$KB (L1 cache resident)

\section{Heap Operations}

\subsection{Initialization}

\begin{lstlisting}[language=Java, basicstyle=\small\ttfamily]
for (int i = 0; i < k; i++) {
    if (iterators[i].hasNext()) {
        T element = iterators[i].next();
        heap.insert(new HeapEntry(element, iterators[i], i));
    }
}
\end{lstlisting}

\textbf{Complexity}: $O(k \log k)$ with naive insertions, $O(k)$ with heapify.

\textbf{Trade-off}: Heapify is more complex but faster for large $k$. For typical $k \leq 100$, naive insertions acceptable.

\subsection{Extract-Min and Refill}

\begin{lstlisting}[language=Java, basicstyle=\small\ttfamily]
public T next() {
    if (heap.isEmpty()) throw new NoSuchElementException();

    HeapEntry<T> min = heap.extractMin();
    T result = min.element;

    if (min.iterator.hasNext()) {
        T nextElement = min.iterator.next();
        heap.insert(new HeapEntry(nextElement, min.iterator, min.index));
    }

    return result;
}
\end{lstlisting}

\textbf{Key Insight}: Refill immediately after extraction maintains heap invariant that each non-exhausted iterator has exactly one element in heap.

\section{Alternative Designs Rejected}

\subsection{Tournament Tree}

\textbf{Rejected because}:
\begin{itemize}
    \item Fewer comparisons ($\log_2 k$ vs $2 \log_2 k$) insufficient to overcome cache penalty
    \item Pointer chasing: Each tree level potentially a cache miss
    \item Implementation complexity: Tree rebuild logic non-trivial
    \item Empirical data (Sedgewick): Heap $2\times$ faster for $k > 10$ on modern CPUs
\end{itemize}

\textbf{When tournament would win}: If comparisons are extremely expensive (e.g., string comparison over network), tournament's fewer comparisons might dominate. Not our use case.

\subsection{Linear Scan}

\textbf{Rejected because}:
\begin{itemize}
    \item $O(Nk)$ unacceptable for $k > 10$
    \item For $k = 100$, N = 1M: $100$M operations vs $20$M for heap (5$\times$ slower)
\end{itemize}

\textbf{When linear would win}: $k \leq 8$ and known at compile time. Could specialize with SIMD min instruction. Out of scope for general-purpose iterator.

\subsection{Pairwise Reduction}

\textbf{Rejected because}:
\begin{itemize}
    \item Violates lazy evaluation (must build full merge tree upfront)
    \item $O(k \log k)$ space for intermediate results
    \item No advantage over heap for sequential processing
\end{itemize}

\textbf{When pairwise would win}: Parallel merge (divide-and-conquer naturally parallelizes). Different problem domain.

\subsection{Sorted Array (Materialize-Then-Sort)}

\textbf{Rejected because}:
\begin{itemize}
    \item Violates lazy evaluation requirement (must consume all iterators upfront)
    \item $O(N)$ space (stores all elements)
    \item $O(N \log N)$ time (worse than $O(N \log k)$ for $k \ll N$)
    \item Breaks streaming semantics (cannot start consuming output until all input consumed)
\end{itemize}

\textbf{When array would win}: If entire dataset must be materialized anyway (e.g., multiple passes needed). Not our use case.

\section{Language-Specific Considerations}

\subsection{Java}

\begin{itemize}
    \item Use \texttt{java.util.PriorityQueue} (backed by array)
    \item Comparator based on HeapEntry.element
    \item Generics: \texttt{CollatingIterator<T extends Comparable<T>>}
    \item Exception handling: NoSuchElementException for next() when empty
\end{itemize}

\subsection{C++}

\begin{itemize}
    \item Use \texttt{std::priority\_queue} with \texttt{std::vector} backing
    \item Custom comparator for HeapEntry
    \item Templates: \texttt{template<typename T> class CollatingIterator}
    \item RAII: Iterators owned or referenced (consider shared\_ptr if ownership unclear)
\end{itemize}

\subsection{Rust}

\begin{itemize}
    \item Use \texttt{std::collections::BinaryHeap} (max-heap by default)
    \item Wrapper type with reversed Ord for min-heap behavior
    \item Traits: \texttt{CollatingIterator<T: Ord>}
    \item Ownership: Iterators moved into CollatingIterator (Rust ownership model)
\end{itemize}

\section{Expected Performance}

Based on design choice and constant factor analysis:

\begin{table}[h]
\centering
\begin{tabular}{lrrr}
\hline
$k$ & $N$ & Expected Time & Time per Element \\ \hline
2   & 1M  & 3M ops        & 3 ops            \\
10  & 1M  & 10M ops       & 10 ops           \\
100 & 1M  & 20M ops       & 20 ops           \\
\hline
\end{tabular}
\caption{Expected operation counts}
\end{table}

At modern CPU speeds ($\sim 3$ GHz, $\sim 1$ comparison/cycle when cached):
\begin{itemize}
    \item $k = 10$, $N = 1$M: $\sim 3$ms processing time
    \item $k = 100$, $N = 1$M: $\sim 7$ms processing time
\end{itemize}

\textbf{Validation}: Empirical measurements (Stage 6) should confirm within $\pm 20\%$.

\section{Summary}

The array-based binary min-heap is selected for:
\begin{itemize}
    \item Asymptotic optimality: $O(N \log k)$
    \item Excellent constants: $\sim 20$ operations per element for $k = 100$
    \item Cache-friendly: Array storage, $\sim 2.4$KB for $k = 100$ (L1 resident)
    \item Simple implementation: Well-known data structure, $\sim 60$ LoC
    \item Scalable: Works well for $k \in [2, 10000]$
\end{itemize}

This design is Pareto-optimal across all evaluation criteria for the general k-way merge problem with lazy evaluation.

\end{document}
